package github.com/hashicorp/terraform-provider-azurerm/internal/services/datafactory
import (
	"fmt"
	"strconv"
	"strings"
	"time"

	"github.com/hashicorp/go-azure-helpers/lang/pointer"
	"github.com/hashicorp/go-azure-sdk/resource-manager/databricks/2022-04-01-preview/workspaces"
	"github.com/hashicorp/go-azure-sdk/resource-manager/datafactory/2018-06-01/factories"
	"github.com/hashicorp/terraform-provider-azurerm/helpers/tf"
	"github.com/hashicorp/terraform-provider-azurerm/internal/clients"
	"github.com/hashicorp/terraform-provider-azurerm/internal/features"
	"github.com/hashicorp/terraform-provider-azurerm/internal/services/datafactory/parse"
	"github.com/hashicorp/terraform-provider-azurerm/internal/services/datafactory/validate"
	"github.com/hashicorp/terraform-provider-azurerm/internal/tf/pluginsdk"
	"github.com/hashicorp/terraform-provider-azurerm/internal/tf/validation"
	"github.com/hashicorp/terraform-provider-azurerm/internal/timeouts"
	"github.com/hashicorp/terraform-provider-azurerm/utils"
	"github.com/jackofallops/kermit/sdk/datafactory/2018-06-01/datafactory" // nolint: staticcheck
)
func resourceDataFactoryLinkedServiceDatabricksCreateUpdate(d *pluginsdk.ResourceData, meta interface{}) error {
	client := meta.(*clients.Client).DataFactory.LinkedServiceClient
	subscriptionId := meta.(*clients.Client).DataFactory.LinkedServiceClient.SubscriptionID
	ctx, cancel := timeouts.ForCreateUpdate(meta.(*clients.Client).StopContext, d)
	defer cancel()

	dataFactoryId, err := factories.ParseFactoryID(d.Get("data_factory_id").(string))
	if err != nil {
		return err
	}

	id := parse.NewLinkedServiceID(subscriptionId, dataFactoryId.ResourceGroupName, dataFactoryId.FactoryName, d.Get("name").(string))

	if d.IsNewResource() {
		existing, err := client.Get(ctx, id.ResourceGroup, id.FactoryName, id.Name, "")
		if err != nil {
			if !utils.ResponseWasNotFound(existing.Response) {
				return fmt.Errorf("checking for presence of existing Data Factory Databricks %s: %+v", id, err)
			}
		}

		if !utils.ResponseWasNotFound(existing.Response) {
			return tf.ImportAsExistsError("azurerm_data_factory_linked_service_azure_databricks", id.ID())
		}
	}

	var databricksProperties *datafactory.AzureDatabricksLinkedServiceTypeProperties

	// Check if the MSI authentication block is set
	msiAuth := d.Get("msi_workspace_id")
	if !features.FivePointOh() {
		if v, ok := d.GetOk("msi_work_space_resource_id"); ok {
			msiAuth = v.(string)
		}
	}
	accessTokenAuth := d.Get("access_token").(string)
	accessTokenKeyVaultAuth := d.Get("key_vault_password").([]interface{})

	// Set the properties based on the authentication type that was provided
	if msiAuth != "" {
		databricksProperties = &datafactory.AzureDatabricksLinkedServiceTypeProperties{
			Authentication:      "MSI",
			WorkspaceResourceID: msiAuth,
		}
	}
	if accessTokenAuth != "" {
		// Assign the access token in the properties block
		databricksProperties = &datafactory.AzureDatabricksLinkedServiceTypeProperties{
			AccessToken: &datafactory.SecureString{
				Value: pointer.To(accessTokenAuth),
				Type:  datafactory.TypeSecureString,
			},
		}
	}

	if len(accessTokenKeyVaultAuth) > 0 && accessTokenKeyVaultAuth[0] != nil {
		databricksProperties = &datafactory.AzureDatabricksLinkedServiceTypeProperties{
			AccessToken: expandAzureKeyVaultSecretReference(accessTokenKeyVaultAuth),
		}
	}

	// Set the other type properties
	databricksProperties.Domain = d.Get("adb_domain").(string)

	if v, ok := d.GetOk("existing_cluster_id"); ok {
		databricksProperties.ExistingClusterID = v.(string)
	}

	if v, ok := d.GetOk("instance_pool"); ok && v.([]interface{})[0] != nil {
		instancePoolMap := v.([]interface{})[0].(map[string]interface{})

		if data := instancePoolMap["instance_pool_id"]; data != nil {
			databricksProperties.InstancePoolID = data
		}

		if data := instancePoolMap["cluster_version"]; data != nil {
			databricksProperties.NewClusterVersion = data
		}

		if minWorkersProperty := instancePoolMap["min_number_of_workers"]; minWorkersProperty != nil {
			maxWorkersProperty := instancePoolMap["max_number_of_workers"]
			if numOfWorkersProperty, err := buildNumberOfWorkersProperties(minWorkersProperty, maxWorkersProperty); err == nil {
				databricksProperties.NewClusterNumOfWorker = numOfWorkersProperty
			} else {
				return fmt.Errorf("expanding `instance_pool`: +%v", err)
			}
		}
	}

	if v, ok := d.GetOk("new_cluster_config"); ok && v.([]interface{})[0] != nil {
		newClusterMap := v.([]interface{})[0].(map[string]interface{})

		if data := newClusterMap["cluster_version"]; data != nil {
			databricksProperties.NewClusterVersion = data
		}

		if minWorkersProperty := newClusterMap["min_number_of_workers"]; minWorkersProperty != nil {
			maxWorkersProperty := newClusterMap["max_number_of_workers"]
			if numOfWorkersProperty, err := buildNumberOfWorkersProperties(minWorkersProperty, maxWorkersProperty); err == nil {
				databricksProperties.NewClusterNumOfWorker = numOfWorkersProperty
			} else {
				return fmt.Errorf("expanding `new_cluster_config`: +%v", err)
			}
		}

		if data := newClusterMap["node_type"]; data != nil {
			databricksProperties.NewClusterNodeType = data
		}

		if data := newClusterMap["driver_node_type"]; data != nil {
			databricksProperties.NewClusterDriverNodeType = data
		}

		if data := newClusterMap["log_destination"]; data != nil {
			databricksProperties.NewClusterLogDestination = data
		}

		if newClusterMap["spark_config"] != nil {
			if sparkConfig := newClusterMap["spark_config"].(map[string]interface{}); len(sparkConfig) > 0 {
				databricksProperties.NewClusterSparkConf = sparkConfig
			}
		}

		if newClusterMap["spark_environment_variables"] != nil {
			if sparkEnvVars := newClusterMap["spark_environment_variables"].(map[string]interface{}); len(sparkEnvVars) > 0 {
				databricksProperties.NewClusterSparkEnvVars = sparkEnvVars
			}
		}

		if newClusterMap["custom_tags"] != nil {
			if customTags := newClusterMap["custom_tags"].(map[string]interface{}); len(customTags) > 0 {
				databricksProperties.NewClusterCustomTags = customTags
			}
		}

		initScripts := newClusterMap["init_scripts"]
		databricksProperties.NewClusterInitScripts = &initScripts
	}

	databricksLinkedService := &datafactory.AzureDatabricksLinkedService{
		Description: pointer.To(d.Get("description").(string)),
		AzureDatabricksLinkedServiceTypeProperties: databricksProperties,
		Type: datafactory.TypeBasicLinkedServiceTypeAzureDatabricks,
	}

	if v, ok := d.GetOk("parameters"); ok {
		databricksLinkedService.Parameters = expandLinkedServiceParameters(v.(map[string]interface{}))
	}

	if v, ok := d.GetOk("integration_runtime_name"); ok {
		databricksLinkedService.ConnectVia = expandDataFactoryLinkedServiceIntegrationRuntime(v.(string))
	}

	if v, ok := d.GetOk("additional_properties"); ok {
		databricksLinkedService.AdditionalProperties = v.(map[string]interface{})
	}

	if v, ok := d.GetOk("annotations"); ok {
		annotations := v.([]interface{})
		databricksLinkedService.Annotations = &annotations
	}

	linkedService := datafactory.LinkedServiceResource{
		Properties: databricksLinkedService,
	}

	if _, err := client.CreateOrUpdate(ctx, id.ResourceGroup, id.FactoryName, id.Name, linkedService, ""); err != nil {
		return fmt.Errorf("creating/updating Data Factory Azure Databricks %s: %+v", id, err)
	}

	d.SetId(id.ID())

	return resourceDataFactoryLinkedServiceDatabricksRead(d, meta)
}
